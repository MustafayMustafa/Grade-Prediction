{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "Using EDA I will analyse the data to identify trends and correlations between variables and the final grade."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Import Student data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Import student data\n",
    "df = pd.read_csv('data/student/x.csv', sep=',')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Checking skewness of the final grade distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot histogram of final grade (G3)\n",
    "plt.hist(df['G3'])\n",
    "plt.xlabel('G3')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Final Grade distribution')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data looks about normally distributed, with a modal score at around 11-12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Feature Selection\n",
    "* Correlation coefficient to determine most useuful variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain variables with correlation to final grade and sort\n",
    "df.corr()['G3'].sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "anomalie: absences show positive correlation, where this should be expected as a negative correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> hot encodeing categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select categorical values only\n",
    "categorical_df = df.select_dtypes('object')\n",
    "\n",
    "# ensure there are no null values\n",
    "categorical_df.isnull()\n",
    "\n",
    "categorical_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode\n",
    "ohe_df = pd.get_dummies(categorical_df)\n",
    "\n",
    "\n",
    "ohe_df['G3'] = df['G3']\n",
    "\n",
    "# get correlations\n",
    "ohe_df.corr()['G3'].sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> return most correlated variables including both numerical and categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return most correlated variables with the final grade\n",
    "# parameters: df - data frame, size - amount of variables to retain\n",
    "def getMostCorrelated(df, size):\n",
    "    # One-Hot Encode Categorical Variables\n",
    "    df = pd.get_dummies(df)\n",
    "    \n",
    "    # Find correlations with the Grade\n",
    "    greatest = df.corr().abs()['G3'].sort_values(ascending=False)\n",
    "    \n",
    "    # retain correlated variables according to size passed\n",
    "    \n",
    "    greatest = greatest[:size+1]\n",
    "    print(greatest)\n",
    "    \n",
    "    df = df.loc[:, greatest.index]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getMostCorrelated(df, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> split the data into training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data for training and testing\n",
    "# split: 25%\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def split_data(df):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df, df['G3'], test_size=0.25, random_state=42)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = split_data(getMostCorrelated(df, ))\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Baseline metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.0\n",
      "[11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0]\n",
      "78     10\n",
      "371    12\n",
      "248     5\n",
      "55     10\n",
      "390     9\n",
      "       ..\n",
      "367     0\n",
      "210     8\n",
      "75     10\n",
      "104    18\n",
      "374    19\n",
      "Name: G3, Length: 99, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# get the median of the final grade\n",
    "median_pred = X_train['G3'].median()\n",
    "print(median_pred)\n",
    "\n",
    "predictions = [median_pred for _ in range(len(X_test))]\n",
    "print(predictions)\n",
    "\n",
    "\n",
    "true_labels = X_test['G3']\n",
    "print(true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "final grade median: 11.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median Baseline  MAE: 3.7879\n",
      "Median Baseline RMSE: 4.8252\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Display the naive baseline metrics\n",
    "\n",
    "baseline_mae = mean_absolute_error(true_labels, predictions)\n",
    "baseline_rmse = mean_squared_error(true_labels, predictions, squared=False)\n",
    "\n",
    "print('Median Baseline  MAE: {:.4f}'.format(baseline_mae))\n",
    "print('Median Baseline RMSE: {:.4f}'.format(baseline_rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> comparison of models against baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
